<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments">
    <meta name="keywords" content="Classical Planning , Embodied AI, Assistive agents, PDDL">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AHSOKA</title>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/bulma.min.css">
    <link rel="stylesheet" href="/assets/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="/assets/css/bulma-slider.min.css">
    <link rel="stylesheet" href="/assets/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="/assets/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="/assets/js/fontawesome.all.min.js"></script>
    <script src="/assets/js/bulma-carousel.min.js"></script>
    <script src="/assets/js/bulma-slider.min.js"></script>
    <!-- <script src="/assets/js/index.js"></script> -->
    <style>
        .content {
            text-align: justify;
        }
    </style>
    
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"> Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
              <a href="http://raraghavarora.github.io/">Raghav Arora</a><sup>1</sup>*,</span>
                            <span class="author-block">
              <a href="http://raraghavarora.github.io/">Shivam Singh</a><sup>1</sup>*,</span>
                            <span class="author-block">
              <a href="https://www.linkedin.com/in/karthik-swaminathan-24644a1a2/">Karthik Swaminathan</a><sup>1</sup>,
            </span>
            <a href="http://adata111.github.io/">Ahana Datta</a><sup>1</sup>,
            </span>
                            <span class="author-block">
              <a href="https://snehasisb.github.io/">Snehasis Banerjee</a><sup>1,2</sup>,
            </span>
                            <span class="author-block">
              <a href="https://sites.google.com/view/brojeshwar/home">Brojeshwar Bhowmick</a><sup>2</sup>,
            </span>
                            <span class="author-block">
              <a href="https://krrish94.github.io/">Krishna Murthy Jatavallabhula</a><sup>3</sup>,
            </span>
                            <span class="author-block">
              <a href="https://www.cs.bham.ac.uk/~sridharm/index.html">Mohan Sridharan</a><sup>4</sup>,
            </span>
                            <span class="author-block">
              <a href="https://faculty.iiit.ac.in/~mkrishna/">Madhava Krishna</a><sup>1</sup>
            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Robotics Research Center, IIIT Hyderabad, India</span><br>
                            <span class="author-block"><sup>2</sup>TCS Research, Tata Consultancy Services, India</span><br>
                            <span class="author-block"><sup>3</sup>MIT CSAIL</span><br>
                            <span class="author-block"><sup>4</sup>Intelligent Robotics Lab, University of Birmingham, UK</span><br>
                            <span class="author-block">*Equal Contribution</span>
                        </div>

                        <div class="column has-text-centered">
                            <span>
                <a href="PDFs/Anticipate_Act__Supplementary_Material.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                                <span>Supplementary Material</span>
                                </a>
                                </span>
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                
                                <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                                <span>Paper</span>
                                </a>
                                </span>
                                <span class="link-block">
                <a href="https://arxiv.org/pdf/2306.01540"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span> -->
                                <!-- <span>arXiv</span>
                                </a>
                                </span> -->
                                <!-- Video Link. -->
                                <span class="link-block">
                <a href="https://youtu.be/Q6V-8bXk8lA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                                <span>Video</span>
                                </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                <a href="https://github.com/AnticipateAndAct/AnticipateAndAct/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                                <span>Code</span>
                                </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <center>
<!--                     <video id="teaser" autoplay muted loop playsinline width="100%">
                    <source src="./assets/images/teaser_video.mp4"
                        type="video/mp4">
                    </video> -->
                    <!-- <img src="assets/images/tsne-f.png"/> -->
                </center>
                <h2 class="subtitle has-text-centered">
                    We introduce <span class="dnerf">Anticipate&Act</span>, a framework for anticipatory action planning leveraging Large Language Models and classical planning using Action Language PDDL.
                </h2>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content">
                        <p>
                            Assistive agents performing household tasks such as making the bed, preparing coffee, or cooking breakfast, often consider one task at a time by computing a plan of actions that accomplishes this task. The agents can be more efficient by anticipating upcoming tasks, and computing and executing an action sequence that jointly achieves these tasks. State of the art methods for task anticipating use data-driven deep network architectures and Large Language Models (LLMs) for task estimation but they do so at the level of high-level tasks and/or require a large number of training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as joint goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework's capabilities in realistic simulated scenarios in the VirtualHome environment and demonstrate a 31% reduction in the execution time in comparison with a system that does not consider upcoming tasks.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Household Domain</h2>
                    <div class="content">
                        <p> We create a household domain in PDDL where an agent has to plan a sequence of finer-granularity actions corresponding to different household tasks like doing laundry, cooking, etc. This domain consists of 33 independent actions, 5 different rooms, 33 objects distributed over 5-10 types, and 19 receptacles. A pictorial representation of the domain is shown below. 
                        Since most common household tasks involve multiple finer-granularity actions, this domain is much more useful for evaluating the performance of our framework than the domains used in previous works. This domain can be further expanded to include more rooms and complexity for more realistic evaluation.
                        </p>
                    </div>
                    <div class="content" style="text-align: center;">
                        <img src="/images/ahsoka/Room_Map1.png"> 
                        <caption> Representation of the household domain.</caption>
                    </div>
                    <h2 class="subtitle has-text-centered">
                    </h2>
                </div>
            </div>
        </div>
    </section>

    <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{agrawal2023clipgraphs,
            title={CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities, 
            author={Ayush Agrawal and Raghav Arora and Ahana Datta and Snehasis Banerjee and Brojeshwar Bhowmick and Krishna Murthy Jatavallabhula and Mohan Sridharan and Madhava Krishna}},
            year={2023},
            eprint={2306.01540},
            archivePrefix={arXiv},
            primaryClass={cs.RO}}</code></pre>
        </div>
    </section> -->


    <footer class="footer">
        <div class="container">
<!--             <div class="content has-text-centered">
                <a class="icon-link" href="https://arxiv.org/abs/2205.10712">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/yashkant" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div> -->
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p align="center">
                            The template for this website was borrowed from <a href="https://nerfies.github.io/">https://nerfies.github.io/</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>
