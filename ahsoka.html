<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments">
    <meta name="keywords" content="Classical Planning , Embodied AI, Assistive agents, PDDL">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AHSOKA</title>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./assets/css/bulma.min.css">
    <link rel="stylesheet" href="./assets/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./assets/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./assets/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./assets/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./assets/js/fontawesome.all.min.js"></script>
    <script src="./assets/js/bulma-carousel.min.js"></script>
    <script src="./assets/js/bulma-slider.min.js"></script>
    <script src="./assets/js/index.js"></script>
    <style>
        .content {
            text-align: justify;
        }
    </style>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"> Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
              <a href="http://raraghavarora.github.io/">Raghav Arora</a><sup>1</sup>*,</span>
                            <span class="author-block">
              <a href="http://raraghavarora.github.io/">Shivam Singh</a><sup>1</sup>*,</span>
                            <span class="author-block">
              <a href="https://www.linkedin.com/in/karthik-swaminathan-24644a1a2/">Karthik Swaminathan</a><sup>1</sup>,
            </span>
            <a href="http://adata111.github.io/">Ahana Datta</a><sup>1</sup>,
            </span>
                            <span class="author-block">
              <a href="https://snehasisb.github.io/">Snehasis Banerjee</a><sup>1,2</sup>,
            </span>
                            <span class="author-block">
              <a href="https://sites.google.com/view/brojeshwar/home">Brojeshwar Bhowmick</a><sup>2</sup>,
            </span>
                            <span class="author-block">
              <a href="https://krrish94.github.io/">Krishna Murthy Jatavallabhula</a><sup>3</sup>,
            </span>
                            <span class="author-block">
              <a href="https://www.cs.bham.ac.uk/~sridharm/index.html">Mohan Sridharan</a><sup>4</sup>,
            </span>
                            <span class="author-block">
              <a href="https://faculty.iiit.ac.in/~mkrishna/">Madhava Krishna</a><sup>1</sup>
            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Robotics Research Center, IIIT Hyderabad, India</span><br>
                            <span class="author-block"><sup>2</sup>TCS Research, Tata Consultancy Services, India</span><br>
                            <span class="author-block"><sup>3</sup>MIT CSAIL</span><br>
                            <span class="author-block"><sup>4</sup>Intelligent Robotics Lab, University of Birmingham, UK</span><br>
                            <span class="author-block">*Equal Contribution</span>
                        </div>

                        <div class="column has-text-centered">
                            <span>
                <a href="PDFs/Anticipate_Act__Supplementary_Material.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                                <span>Supplementary Material</span>
                                </a>
                                </span>
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                
                                <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                                <span>Paper</span>
                                </a>
                                </span>
                                <span class="link-block">
                <a href="https://arxiv.org/pdf/2306.01540"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span> -->
                                <!-- <span>arXiv</span>
                                </a>
                                </span> -->
                                <!-- Video Link. -->
                                <!-- <span class="link-block">
                <a href="https://youtu.be/aZgyFGuWZbk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span> -->
                                <!-- <span>Video</span>
                                </a>
                                </span> -->
                                <!-- Code Link. -->
                                <!-- <span class="link-block">
                <a href="https://github.com/CLIPGraphs/CLIPGraphs.github.io/tree/irona"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span> -->
                                <!-- <span>Code</span>
                                </a>
                                </span> -->
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <center>
<!--                     <video id="teaser" autoplay muted loop playsinline width="100%">
                    <source src="./assets/images/teaser_video.mp4"
                        type="video/mp4">
                    </video> -->
                    <!-- <img src="assets/images/tsne-f.png"/> -->
                </center>
                <h2 class="subtitle has-text-centered">
                    We introduce <span class="dnerf">Anticipate&Act</span>, a framework for anticipatory action planning leveraging Large Language Models and classical planning using Action Language PDDL.
                </h2>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content">
                        <!-- <p>
                            This paper introduces a novel method for determining the best room to place an object in, for embodied scene rearrangement. While state-of-the-art approaches rely on large language models (LLMs) or reinforcement learned (RL) policies for this task, our approach, CLIPGraphs, efficiently combines commonsense domain knowledge, data-driven methods, and recent advances in multimodal learning. Specifically, it (a) encodes a knowledge graph of prior human preferences about the room location of different objects in home environments, (b) incorporates vision-language features to support multimodal queries based on images or text, and (c) uses a graph network to learn object-room affinities based on embeddings of the prior knowledge and the vision-language features.
                            We demonstrate that our approach provides better estimates of the most appropriate location of objects from a benchmark set of object categories in comparison with state-of-the-art baselines
                        </p> -->
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <!-- <section class="section">
        <div class="container is-max-desktop">
            Abstract.
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Dataset</h2>
                    <div class="content">
                        <p> The <span class="dnerf">IRONA</span> 30 web-scraped images for each of the 268 object categories used by <a href="https://yashkant.github.io/housekeep/">Housekeep</a>. With over 8000 images spanning over 268 common household item object categories, the IRONA dataset serves as a diverse and reliable dataset for our approach.
                        </p>
                    </div>
                    <div class="content">
                        <img src="assets/images/gandr-collage.jpg"> The above image is small subset of the IRONA dataset.
                    </div>
                    <h2 class="subtitle has-text-centered">
                    </h2>
                </div>
            </div>
        </div>
    </section> -->

    <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{agrawal2023clipgraphs,
            title={CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities, 
            author={Ayush Agrawal and Raghav Arora and Ahana Datta and Snehasis Banerjee and Brojeshwar Bhowmick and Krishna Murthy Jatavallabhula and Mohan Sridharan and Madhava Krishna}},
            year={2023},
            eprint={2306.01540},
            archivePrefix={arXiv},
            primaryClass={cs.RO}}</code></pre>
        </div>
    </section> -->


    <footer class="footer">
        <div class="container">
<!--             <div class="content has-text-centered">
                <a class="icon-link" href="https://arxiv.org/abs/2205.10712">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/yashkant" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div> -->
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p align="center">
                            The template for this website was borrowed from <a href="https://nerfies.github.io/">https://nerfies.github.io/</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>
